import BlackPoint from "../components/BlackPoint"
import ImageContainer from "../components/ImageContainer/ImageContainer"
import InfoBox from "../components/InfoBox/InfoBox"
import { NumberPoint } from "../components/NumberPoint/NumberPoint"
import Title from "../components/Text/Title"

const Learning = () => {
	return (
		<div className="sc-text-5 sc-gap-6 flex flex-col text-[#464646]">
			<Title id="rl-reasoning">Обучение с подкреплением и ризонинг</Title>

			<div className="sc-gap-6 flex">
				<div>
					<ImageContainer
						fit="cover"
						src={"/images/image-68.png"}
						className="sc-w-[244px] relative h-full"
					/>
				</div>
				<div className="sc-gap-4 flex flex-col">
					<p>
						Файнтюнинг отлично справляется с задачей адаптации модели
						под конкретные сценарии. Однако у него есть важное ограничение:
						файнтюнинг учит модель воспроизводить правильные ответы из заранее
						подготовленных данных, но не всегда даёт ей понимание, какие ответы
						действительно «хорошие», полезные и подходящие с точки зрения
						человека.
					</p>
					<p>
						Поэтому в индустрии после этапа файнтюнинга на сцену обычно выходит 
						<strong>
							Reinforcement Learning (RL) — обучение с подкреплением.
						</strong>
						В отличие от простого файнтюнинга, RL не просто показывает модели
						примеры хороших ответов, а напрямую даёт ей обратную связь о том,
						насколько её ответ был полезным или верным. Благодаря этому модель
						постепенно учится понимать, какие генерации наиболее подходящие
						и почему.
					</p>
				</div>
			</div>
			<p>
				Особенно ярко RL проявил себя как основной инструмент в обучении
				ризонинг-моделей, таких как o3 или DeepSeek R1. Ризонинг — не просто
				очередной тренд, это новая парадигма масштабирования LLM, которая
				помогает моделям не только воспроизводить знания, но и использовать их,
				решая сложные задачи пошагово и логично.
			</p>
			<p>
				В этой главе мы подробно разберёмся, как работает RL, и как он помогает
				LLM выйти на совершенно новый уровень.
			</p>
			<div className="sc-gap-3 flex flex-col">
				<h3 className="sc-text-8 font-bold text-[#151515]">
					Основы обучения с подкреплением
				</h3>
				<p>В RL есть две основные составляющие:</p>
				<div className="sc-gap-3 flex items-center">
					<NumberPoint>1</NumberPoint> Агент
				</div>
				<div className="sc-gap-3 flex items-center">
					<NumberPoint>2</NumberPoint> Среда
				</div>
			</div>
			<div className="sc-gap-5 flex">
				<div className="sc-gap-4 flex flex-col justify-end">
					<div>
						<ImageContainer
							src={"/images/image-69.png"}
							className="sc-w-[510px] sc-h-[248px] relative"
						/>
					</div>
					<div>
						<InfoBox arrowPosition="top" arrowOffset="70px">
							<p className="sc-p-4">
								После получения обратной связи от среды агент корректирует своё
								поведение (это называется <strong>политикой</strong>) так,
								чтобы в будущем получать больше положительных наград.
							</p>
						</InfoBox>
					</div>
				</div>
				<div className="sc-gap-4 flex flex-col">
					<InfoBox arrowPosition="left" arrowOffset="86px">
						<p className="sc-p-4">
							Агент (в нашем случае это языковая модель) совершает некоторое
							действие.
						</p>
					</InfoBox>
					<div className="relative">
						<div className="relative">
							<svg
								className="sc-w-[221px] sc-h-[35px] sc-top-[100px] -sc-left-[229px] absolute z-5"
								viewBox="0 0 221 35"
								fill="none"
								xmlns="http://www.w3.org/2000/svg"
							>
								<path
									d="M220 19.333C217.1 19.333 214.945 16.8665 212.581 15.4392C206.601 11.8283 200.663 8.08544 194.16 5.44276C179.651 -0.453967 161.171 -1.9815 149.534 10.3385C145.474 14.6365 139.545 21.579 142.435 28.1454C143.877 31.4228 147.597 33.9704 151.223 33.6559C156.47 33.2008 157.198 26.1576 157.729 22.2249C158.409 17.1797 155.163 12.8539 150.744 10.7256C147.161 9.00043 142.551 9.08611 138.645 9.08611C134.993 9.08611 131.447 11.0178 128.373 12.8661C121.145 17.2125 114.141 22.5028 106.505 26.0732C95.0981 31.4073 83.0318 33.6231 70.4847 33.6787C58.814 33.7305 46.3072 34.8169 34.7836 32.6768C26.9532 31.2226 17.5464 29.4367 10.9525 24.7525C5.62794 20.9701 1 15.8672 1 9.08611"
									stroke="black"
									strokeWidth={2}
									strokeLinecap="round"
									strokeDasharray="4 4"
								/>
							</svg>
						</div>

						<InfoBox arrowPosition="left" arrowOffset="115px">
							<div className="sc-p-4 sc-gap-4 flex flex-col">
								<p>
									Далее среда каким-то образом “реагирует” на действие агента,
									выдавая ему некоторую <strong>награду (reward).</strong>{" "}
									Награда можем быть и положительной, и отрицательной.
								</p>
								<p>
									В случае LLM среда – это любой внешний оценщик. Чаще всего это
									специальная модель, имитирующая оценки человека (ее обычно
									называют reward-модель).
								</p>
							</div>
						</InfoBox>
					</div>
				</div>
			</div>
			<div className="sc-p-4 bg-myblue sc-text-4 rounded-lg border font-semibold text-[#151515]">
				<p>Например:</p>
				<ol className="list-decimal">
					<div className="sc-px-4">
						<li>
							Агент (LLM) генерирует ответ на вопрос человека про столицу
							Франции и отвечает «Марсель».
						</li>
					</div>
					<div className="sc-px-4">
						<li>
							Среда (человек или reward-модель) оценивает ответ как неправильный
							и выдаёт отрицательную награду.
						</li>
					</div>
					<div className="sc-px-4">
						<li>
							Агент корректирует свои параметры так, чтобы в будущем с большей
							вероятностью отвечать «Париж».
						</li>
					</div>
				</ol>
			</div>
			<div className="sc-p-4 bg-myblue sc-text-4 sc-gap-4 flex flex-col rounded-lg border font-semibold text-[#151515]">
				<p>
					Так как в случае LLM награда в основном задается через оценки
					человека, подход называют RLHF (Reinforcement Learning with Human
					Feedback).
				</p>
				<p>
					В отличие от классического обучения с подкреплением (RL), где модель
					обычно получает численные награды из заранее определённой функции
					вознаграждения, в RLHF модель учится генерировать ответы, опираясь на
					человеческие предпочтения.
				</p>
			</div>
			<div>
				<ImageContainer
					fit="cover"
					src={"/images/image-70.png"}
					className="sc-w-[952px] sc-h-[291px] relative"
				/>
			</div>
			<div className="sc-gap-2 flex flex-col">
				<div className="sc-gap-3 flex items-center text-[#151515]">
					<span className="sc-text-[14px] sc-px-[10px] sc-py-[3px] rounded-[40px] bg-[#151515] text-white">
						Этап 1
					</span>
					<span>Сбор разметки данных от людей</span>
				</div>
				<p>
					На практике наиболее распространён подход, при котором для разметки
					люди выбирают из нескольких вариантов ответов модели на один и тот же
					вопрос (это может быть парное сравнение или ранжирование). Можно также
					собирать численные или бинарные оценки (нравится / не нравится /
					нравится на 4 из 10).
				</p>
			</div>
			<div className="sc-gap-2 flex flex-col">
				<div className="sc-gap-3 flex items-center text-[#151515]">
					<span className="sc-text-[14px] sc-px-[10px] sc-py-[3px] rounded-[40px] bg-[#151515] text-white">
						Этап 2
					</span>
					<span>Обучение Reward модели</span>
				</div>
				<p>
					Размеченные человеком данные используются для обучения модели
					вознаграждений. Она учится воспроизводить человеческие оценки, то есть
					автоматически предсказывать, какие ответы люди предпочитают больше
					других.
				</p>
			</div>
			<div className="sc-gap-2 flex flex-col">
				<div className="sc-gap-3 flex items-center text-[#151515]">
					<span className="sc-text-[14px] sc-px-[10px] sc-py-[3px] rounded-[40px] bg-[#151515] text-white">
						Этап 3
					</span>
					<span>Обучение политики</span>
				</div>
				<p>
					Теперь уже модель вознаграждений используется для дальнейшего обучения
					основной языковой модели. LLM генерирует ответы и получает от reward
					модели численные награды, на основании которых постепенно улучшает
					свои ответы.
				</p>
			</div>
			<div className="sc-gap-2 flex flex-col">
				<div className="sc-gap-3 flex items-center text-[#151515]">
					<span className="sc-text-[14px] sc-px-[10px] sc-py-[3px] rounded-[40px] bg-[#151515] text-white">
						Этап 4
					</span>
					<span>Цикл обратной связи</span>
				</div>
				<p>
					Лучшие ответы модели могут снова направляться к человеку для
					дополнительной оценки и валидации. Так можно постепенно собирать
					больше данных и улучшать reward модель.
				</p>
			</div>
			<div className="sc-p-4 bg-myblue sc-text-4 sc-gap-4 flex flex-col rounded-lg border font-semibold text-[#151515]">
				<p>
					RLHF используется для того, чтобы языковая модель лучше
					соответствовала ожиданиям и предпочтениям человека. Такой процесс
					называют элайментом (alignment, то есть «выравнивание» поведения
					модели с человеческими ценностями и пожеланиями).
				</p>
			</div>
			<div className="sc-p-4 bg-myblue sc-text-4 sc-gap-4 flex flex-col rounded-lg border font-semibold text-[#151515]">
				<p>
					Именно благодаря RLHF языковые модели получаются такими “человечными”
					и умеют, например, вести вежливый диалог или “проявлять” эмпатию. Все
					потому что модель обучается не просто предсказывать текст, а учитывает
					оценки и предпочтения людей.
				</p>
				<p>
					Также RLHF незаменим, когда мы говорим про безопасность и адекватность
					модели. Он помогает минимизировать нежелательные или вредные
					результаты генераций.
				</p>
			</div>
			<div>
				<p>
					Мы обговорили общую схему RLHF, но еще не обсудили, как именно мы
					обучаем политику на третьем (основном) шаге.
				</p>
				<div className="m-auto flex w-8/10 items-center">
					<p className="sc-p-3 sc-text-4 sc-w-[264px] border">
						На практике существует множество алгоритмов для обучения политики,
						однако чаще всего используют эти три подхода:
					</p>
					<div>
						<svg
							className="sc-w-[142px] sc-h-[119px]"
							viewBox="0 0 142 119"
							fill="none"
							xmlns="http://www.w3.org/2000/svg"
						>
							<path
								d="M0 59L80.5313 59C83.845 59 86.5312 56.3137 86.5312 53L86.5312 7C86.5312 3.68629 89.2175 0.999996 92.5312 0.999996L142 0.999993"
								stroke="#151515"
								strokeWidth={2}
							/>
							<path
								d="M1.85333e-06 59L142 59"
								stroke="#151515"
								strokeWidth={2}
							/>
							<path
								d="M0 59L80.5313 59C83.845 59 86.5312 61.6863 86.5312 65L86.5312 112C86.5312 115.314 89.2175 118 92.5312 118L142 118"
								stroke="#151515"
								strokeWidth={2}
							/>
						</svg>
					</div>
					<div className="sc-text-4 sc-gap-3 flex flex-col">
						<p className="sc-py-3 sc-p-6 rounded-[48px] bg-black text-center text-white">
							DPO (Direct Preference Optimization)
						</p>
						<p className="sc-py-3 sc-p-6 rounded-[48px] bg-black text-center text-white">
							PPO (Proximal Policy Optimization)
						</p>
						<p className="sc-py-3 sc-p-6 rounded-[48px] bg-black text-center text-white">
							GRPO (Guided Reward Policy Optimization)
						</p>
					</div>
				</div>
			</div>
			<div>
				<h4 className="font-medium text-[#151515]">
					PPO (Proximal Policy Optimization)
				</h4>
				<p>
					Основной и наиболее широко используемый в RLHF алгоритм обучения с
					подкреплением, <strong>представленный</strong> командой OpenAI в 2017
					году (сразу после того, как они же написали оригинальную работу про
					RLHF). Алгоритм быстро стал популярным благодаря своей простоте и
					надежности и используется по сей день.
				</p>
			</div>
			<div>
				<div className="sc-gap-3 flex">
					<div>
						<ImageContainer
							fit="contain"
							src={"/images/image-71.png"}
							className="sc-w-[637px] relative h-full"
						/>
					</div>
					<div>
						<ol className="flex w-9/10 list-decimal flex-col font-medium">
							<div className="sc-pl-8 sc-pr-3 sc-py-3">
								<li>
									Все пройденные «траектории», то есть накопленный опыт,
									хранятся в формате списков (состояние → действие → награда →
									следующее состояние).
								</li>
							</div>
							<div className="sc-pl-8 sc-pr-3 sc-py-3">
								<li>
									Среда принимает от актора действия, а обратно возвращает
									награду rt+1 и следующее состояние st+1
								</li>
							</div>
							<div className="sc-pl-8 sc-pr-3 sc-py-3">
								<li>
									Среда принимает от актора действия, а обратно возвращает
									награду rt+1 и следующее состояние st+1
								</li>
							</div>
						</ol>
					</div>
				</div>
				<ol className="flex w-9/10 list-decimal flex-col font-medium" start={4}>
					<div className="sc-pl-8 sc-pr-3 sc-py-3">
						<li>
							Обновляем параметры моделей (и актора, и критика) с помощью
							обычного градиентного спуска.
						</li>
					</div>
					<div className="sc-pl-8 sc-pr-3 sc-py-3">
						<li>
							Функция потерь для обновления главной модели-актора. Обратите
							внимание, что PPO вводит коэффициент «clip», который не позволяет
							новой политике πθ​ слишком сильно отличаться от старой
							политики πθold.
						</li>
					</div>{" "}
					<div className="sc-pl-8 sc-pr-3 sc-py-3">
						<li>
							Это так называемое <strong>преимущество</strong> (может быть и
							положительным, и отрицательным). Его мы вычисляем, сравнивая
							фактическую награду с ожидаемой ценностью состояния от Critic.
						</li>
					</div>
				</ol>
			</div>
			<div>
				<p>PPO работет в формате Actor-Critic.</p>
				<div className="sc-text-4 font-semibold text-[#151515]">
					<p className="sc-gap-2 font-onest flex items-baseline font-medium">
						<BlackPoint />
						Actor – это наша модель, то есть текущая политика, которая совершает
						действие at, исходя из текущего состояния st (то есть входных
						данных).
					</p>
					<p className="sc-gap-2 font-onest flex items-baseline font-medium">
						<BlackPoint />
						Critic (или value model) – модель, которая оценивает долгосрочную
						ожидаемую выгоду от политики в текущем состоянии. Это позволяет
						определить, насколько фактическая награда за действие актора
						оказалась лучше или хуже ожиданий, и обновлять политику, исходя из
						этой информации.
					</p>
				</div>
			</div>
			<div className="sc-p-4 bg-myblue sc-text-4 sc-gap-4 flex flex-col rounded-lg border font-semibold text-[#151515]">
				<p>
					Вы можете задаться вполне логичным вопросом «Зачем нам вообще
					модель-критик? Разве нельзя оценивать политику просто по награде?».
					Нет, в контексте LLM, к сожалению нельзя.
				</p>
				<p>
					Представим, что мы получили награду, и она равна 5. Это хорошо
					или плохо? Мы не знаем. 5 баллов может быть отличной наградой,
					если обычно в данном состоянии мы получаем только 1–2 балла.
					Или это может быть очень плохим результатом, если обычно мы получали
					в этом состоянии по 10–15 баллов.
				</p>
				<p>
					Критик как раз задаёт эталон — он говорит, чего стоит ожидать в каждом
					конкретном состоянии. После этого мы можем сказать, что 5 баллов —
					это лучше или хуже ожидаемого, то есть хорошо, или плохо. Только таким
					образом можно эффективно обучать актор.
				</p>
			</div>
			<div className="sc-gap-2 flex flex-col">
				<p className="text-[#151515]">
					GRPO (Guided Reward Policy Optimization)
				</p>
				<p>
					Однако у PPO есть и свои минусы. В частности, та самая модель-критик
					на самом деле очень требовательна с точки зрения ресурсов, поскольку
					ее тоже нужно обучать. Это сильно тормозит весь процесс и делает
					обучение дорогим.
				</p>
				<p>
					Как обойти этот нюанс несколько лет назад придумали исследователи из
					DeepSeek. Они предложили алгоритм GRPO – более эффективную вариацию
					PPO, которая быстро набрала популярность.
				</p>
			</div>
			<div>
				<ImageContainer
					fit="cover"
					src={"/images/image-72.png"}
					className="sc-w-full sc-h-[413px] relative"
				/>
			</div>
			<ol className="flex w-9/10 list-decimal flex-col font-medium" start={4}>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>Модель генерирует уже не один ответ, а сразу несколько.</li>
				</div>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>
						В GRPO из алгоритма вообще <strong>удалена Value Model</strong>, то
						есть модель-критик. Вместо этого, чтобы правильно оценивать награды,
						мы используем среднюю награду от группы ответов на один и тот же
						вопрос, и так определяем, насколько &quot;хороши&quot; действия
						модели. 
					</li>
				</div>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>
						Каждый ответ из группы получает свою награду. За счёт этого вместо
						того, чтобы сравнивать награду определённого действия с ожиданиями
						модели-критика, мы можем сравнить ту же награду со средней наградой
						по группе.
					</li>
				</div>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>
						Политика обновляется на основе относительных преимуществ внутри
						группы.
					</li>
				</div>
			</ol>
			<div className="sc-gap-2 flex flex-col">
				<p className="text-[#151515]">DPO (Direct Preference Optimization)</p>
				<p>
					Ещё один сравнительно новый алгоритм, предложенный в 2023 году
					как более простой и эффективный подход к RLHF. Его ключевое отличие
					от PPO и GRPO заключается в том, что DPO вообще не требует отдельной
					reward-модели и явного этапа обучения с подкреплением: он напрямую
					оптимизирует политику, используя пары ответов, ранжированных людьми
					по степени предпочтения.
				</p>
				<p>
					Как обойти этот нюанс несколько лет назад придумали исследователи из
					DeepSeek. Они предложили алгоритм GRPO – более эффективную вариацию
					PPO, которая быстро набрала популярность.
				</p>
			</div>
			<div>
				<ImageContainer
					fit="cover"
					src={"/images/image-73.png"}
					className="sc-w-full sc-h-[356px] relative"
				/>
			</div>
			<ol className="flex w-9/10 list-decimal flex-col font-medium" start={4}>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>
						То есть, в отличие от классических подходов к RLHF, в DPO мы
						полностью избегаем обучения модели вознаграждений и этапа обучения с
						подкреплением как такового. Вместо этого DPO напрямую оптимизирует
						модель (политику) по небольшому множеству данных о человеческих
						предпочтениях, заданных через пары ответов.
					</li>
				</div>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>
						Для каждого запроса из датасета у нас есть два варианта ответа
						модели, один из которых люди посчитали предпочтительнее другого.
					</li>
				</div>
				<div className="sc-pl-8 sc-pr-3 sc-py-3">
					<li>
						Эти пары используются напрямую для оптимизации модели. Целью
						становится максимизация вероятности предпочтительных ответов и
						минимизация вероятности менее предпочтительных. Другими словами,
						модель напрямую учится выбирать ответы, наиболее соответствующие
						человеческим предпочтениям, без необходимости явно строить отдельную
						модель вознаграждений или вычислять преимущества через критика.
					</li>
				</div>
			</ol>
			<p>
				Итак, мы разобрались с тем, как работает RL, RLHF и три основных
				алгоритма обучения политики.
			</p>
			<p>
				Однако сегодня RL используется не только для улучшения качества ответов
				и соответствия человеческим предпочтениям. Все, что вы увидели сверху,
				сейчас имеет неоценимое значение для индустрии в первую очередь
				из-за ризонинга.
			</p>
			<div className="sc-gap-2 flex">
				<div>
					<ImageContainer
						fit="cover"
						src={"/images/image-74.png"}
						className="sc-w-[251px] relative h-full"
					/>
				</div>
				<div>
					<InfoBox arrowOffset="89px" arrowPosition="left">
						<div className="sc-p-4">
							<p>
								Ризонинг-модели отличаются от обычных тем, что у них
								есть так называемые цепочки мыслей (chain of thought).
								Считается, что такие модели способны в каком‑то смысле думать,
								то есть они не просто воспроизводят информацию, а способны
								пошагово «рассуждать» и обосновывать свои ответы.
							</p>
							<p>
								RL в обучении ризонинг-моделей играет ключевую роль: в процессе
								обучения модель получает награду не просто за правильный ответ,
								а за логичную и последовательную цепочку рассуждений, которая
								к нему приводит.
							</p>
						</div>
					</InfoBox>
				</div>
			</div>
			<div className="sc-p-4 bg-myblue sc-text-4 sc-gap-4 flex flex-col rounded-lg border font-semibold text-[#151515]">
				<p>
					Самое интересное в ризонинге то, что это новая парадигма
					масштабирования LLM.
				</p>
				<p>
					Некоторое время назад считалось, что LLM могут становится умнее только
					за счет увеличения количества обучающих данных и ресурсов на обучения.
					Однако некоторое время назад, когда появились первые большие
					ризонинг-модели (o1 или R1), обнаружилось, что ризонинг тоже способен
					масштабировать модели.
				</p>
				<p>
					То есть чем дольше модель рассуждает и чем больше токенов вкладывает в
					свои цепочки мыслей, тем качественнее получаются ответы. Так как это
					масштабирование происходит только за счет и во время инференса, его
					называют test-time scailng.
				</p>
			</div>
			<div>
				<ImageContainer
					fit="cover"
					src={"/images/image-75.png"}
					className="sc-h-[554px] relative w-full"
				/>
			</div>
			<p>
				Эти графики демонстрируют test-time scailng для модели o1 от OpenAI. Тут
				явно показано, что чем больше вычислений модель тратит на рассуждения во
				время инференса, тем выше метрики итоговых ответов.
			</p>
			<p className="sc-p-4 sc-text-4 rounded-lg border font-semibold text-[#151515]">
				Многие (в том числе некоторые крупные ученые и лидеры индустрии)
				считают, что именно ризонинг может привести нас к AGI. Так это или нет –
				проверим через несколько лет.
			</p>
		</div>
	)
}

export default Learning
